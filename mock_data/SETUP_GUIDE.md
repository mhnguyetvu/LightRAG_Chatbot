# üêøÔ∏è SQUIRREL MINI ‚Äî SETUP GUIDE
## Ph√¢n chia Local (M1 Pro) vs RunPod (RTX 4090)

---

## T·ªîNG QUAN CHI PH√ç

| Giai ƒëo·∫°n | Ch·∫°y ·ªü ƒë√¢u | Th·ªùi gian b·∫≠t RunPod | Chi ph√≠ RunPod |
|---|---|---|---|
| Phase 1: Setup m√¥i tr∆∞·ªùng | Local | 0 | $0 |
| Phase 2: Chu·∫©n b·ªã mock data | Local | 0 | $0 |
| Phase 3: Indexing v√†o LightRAG | **RunPod** | ~2-3 gi·ªù | ~$1.5-2 |
| Phase 4: Dev & Test features | Local + OpenAI API | 0 | ~$3-5 t·ªïng API |
| Phase 5: Demo | Local serve | 0 | $0 |
| **T·ªîNG** | | **~3 gi·ªù RunPod** | **~$5-7** |

> üí° **Chi·∫øn l∆∞·ª£c ch√≠nh:** Ch·ªâ b·∫≠t RunPod ƒë·ªÉ index data (n·∫∑ng, c·∫ßn GPU cho embedding nhanh).
> Sau khi index xong, data l∆∞u v√†o storage local ‚Üí query b√¨nh th∆∞·ªùng qua OpenAI API (r·∫ª h∆°n).

---

## PHASE 1 ‚Äî SETUP LOCAL (M1 Pro)

### 1.1 Install dependencies

```bash
# T·∫°o virtual environment
python -m venv .venv
source .venv/bin/activate

# Install LightRAG
pip install "lightrag-hku[api]"

# Install th√™m
pip install fastapi uvicorn streamlit python-dotenv textract
```

### 1.2 Setup PostgreSQL local (d√πng Docker)

```bash
# Pull v√† ch·∫°y PostgreSQL v·ªõi pgvector + AGE
docker run -d \
  --name squirrel-postgres \
  -e POSTGRES_USER=squirrel \
  -e POSTGRES_PASSWORD=squirrel123 \
  -e POSTGRES_DB=squirrel_rag \
  -p 5432:5432 \
  gzdaniel/postgres-for-rag:latest

# Verify
docker ps
```

### 1.3 T·∫°o file .env

```bash
# .env
LLM_BINDING=openai
LLM_MODEL=gpt-4o-mini
OPENAI_API_KEY=sk-xxx-your-key

EMBEDDING_BINDING=openai
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIM=1536

# PostgreSQL
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=squirrel
POSTGRES_PASSWORD=squirrel123
POSTGRES_DATABASE=squirrel_rag

# Storage backends (d√πng PostgreSQL all-in-one)
KV_STORAGE=PGKVStorage
VECTOR_STORAGE=PGVectorStorage
GRAPH_STORAGE=PGGraphStorage
DOC_STATUS_STORAGE=PGDocStatusStorage

# Vietnamese language
LIGHTRAG_LANGUAGE=Vietnamese
```

### 1.4 Start LightRAG Server (test)

```bash
lightrag-server --host 0.0.0.0 --port 9621
# Truy c·∫≠p: http://localhost:9621 ƒë·ªÉ xem Web UI
```

---

## PHASE 2 ‚Äî SETUP RUNPOD (Ch·ªâ khi c·∫ßn index)

### 2.1 T·∫°o RunPod Pod

Tr√™n RunPod dashboard:
- **GPU:** RTX 4090 (24GB)
- **Template:** RunPod PyTorch 2.x (ho·∫∑c Ollama template)
- **Disk:** 50GB (ƒë·ªß ch·ª©a Qwen2.5:32B ~20GB + BGE-M3 ~1.5GB)
- **Expose ports:** 11434 (Ollama), 8888 (Jupyter n·∫øu c·∫ßn)

### 2.2 Setup Ollama tr√™n RunPod

```bash
# SSH v√†o RunPod
ssh -i ~/.ssh/id_rsa root@<runpod-ip> -p <port>

# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull models (ch·∫°y background, t·ªën 15-30 ph√∫t)
ollama pull qwen2.5:32b &        # ~20GB ‚Äî LLM ch√≠nh
ollama pull bge-m3 &              # ~1.5GB ‚Äî Embedding ti·∫øng Vi·ªát

# Expose Ollama ra ngo√†i (m·∫∑c ƒë·ªãnh ch·ªâ localhost)
OLLAMA_HOST=0.0.0.0:11434 ollama serve
```

### 2.3 Test k·∫øt n·ªëi t·ª´ Local

```bash
# Test t·ª´ m√°y local
curl http://<runpod-ip>:<exposed-port>/api/tags
# Ph·∫£i th·∫•y list models: qwen2.5:32b, bge-m3
```

### 2.4 Ch·∫°y ingest script t·ª´ Local ‚Üí RunPod

```bash
# Clone project v·ªÅ local (n·∫øu ch∆∞a c√≥)
cd squirrel_mini

# Ch·∫°y ingest v·ªõi Ollama tr√™n RunPod
OLLAMA_BASE_URL=http://<runpod-ip>:<port> python scripts/ingest.py \
  --llm ollama \
  --ollama-url http://<runpod-ip>:<port> \
  --data-dir ./squirrel_mock_data

# Ho·∫∑c n·∫øu mu·ªën nhanh h∆°n, d√πng OpenAI:
OPENAI_API_KEY=sk-xxx python scripts/ingest.py --llm openai
```

> ‚è±Ô∏è **∆Ø·ªõc t√≠nh th·ªùi gian indexing mock data n√†y:**
> - V·ªõi OpenAI GPT-4o-mini: ~15-20 ph√∫t (~$0.30-0.50)
> - V·ªõi Qwen2.5:32B tr√™n 4090: ~45-60 ph√∫t (free sau khi setup)

### 2.5 T·∫Øt RunPod sau khi index xong!

Data ƒë√£ l∆∞u v√†o PostgreSQL local ‚Üí RunPod kh√¥ng c·∫ßn n·ªØa cho dev phase.

```bash
# Tr√™n RunPod dashboard ‚Üí Stop pod
# Ho·∫∑c: d√πng RunPod CLI
runpodctl stop pod <pod-id>
```

---

## PHASE 3 ‚Äî BUILD FEATURES (Local)

Sau khi c√≥ data trong LightRAG, build c√°c t√≠nh nƒÉng:

### 3.1 T√≥m t·∫Øt tin nh·∫Øn ch∆∞a ƒë·ªçc

```python
# feature/summarize.py
async def summarize_unread(channel: str, messages: list[dict], user: str, rag: LightRAG):
    # Insert tin nh·∫Øn m·ªõi v√†o RAG tr∆∞·ªõc (ƒë·ªÉ c·∫≠p nh·∫≠t KB)
    text = format_messages(messages)
    await rag.ainsert(text, ids=[f"chat_{channel}_{today}"])

    # Query t√≥m t·∫Øt
    return await rag.aquery(
        f"T√≥m t·∫Øt c√°c tin nh·∫Øn ch∆∞a ƒë·ªçc trong k√™nh {channel}. "
        f"Highlight: quy·∫øt ƒë·ªãnh quan tr·ªçng, task ƒë∆∞·ª£c giao, deadline, v·∫•n ƒë·ªÅ c·∫ßn x·ª≠ l√Ω.",
        param=QueryParam(
            mode="naive",  # naive ƒë·ªß d√πng cho recent messages
            user_prompt="Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát. Format: [T√≥m t·∫Øt 2-3 c√¢u] + [Bullet points: actions/decisions]"
        )
    )
```

### 3.2 B√°o c√°o c√¥ng vi·ªác theo ng√†y

```python
async def daily_report(user: str, date: str, rag: LightRAG):
    return await rag.aquery(
        f"T·ªïng h·ª£p to√†n b·ªô c√¥ng vi·ªác c·ªßa {user} trong ng√†y {date}: "
        f"tasks ƒë√£ ho√†n th√†nh, ƒëang l√†m, g·∫∑p v·∫•n ƒë·ªÅ g√¨, ƒë∆∞·ª£c giao task m·ªõi n√†o.",
        param=QueryParam(
            mode="hybrid",
            user_prompt="Tr·∫£ l·ªùi ti·∫øng Vi·ªát. Format b√°o c√°o EOD: ‚úÖ Done / üîÑ In Progress / ‚ùå Blocked / üìå New Tasks"
        )
    )
```

### 3.3 Q&A n·ªôi b·ªô

```python
async def internal_qa(question: str, rag: LightRAG):
    return await rag.aquery(
        question,
        param=QueryParam(
            mode="local",
            user_prompt=(
                "Tr·∫£ l·ªùi d·ª±a tr√™n t√†i li·ªáu n·ªôi b·ªô c√¥ng ty TechViet. "
                "N·∫øu kh√¥ng c√≥ th√¥ng tin ƒë·ªß, h√£y n√≥i r√µ v√† g·ª£i √Ω li√™n h·ªá ai. "
                "Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát."
            )
        )
    )
```

### 3.4 Ch·∫°y Streamlit UI

```bash
streamlit run app.py
```

---

## QUICK COMMANDS CHEATSHEET

```bash
# B·∫≠t PostgreSQL (m·ªói l·∫ßn restart Mac)
docker start squirrel-postgres

# Start LightRAG server
source .venv/bin/activate
lightrag-server --port 9621

# Ch·∫°y Streamlit
streamlit run app.py --server.port 8501

# Ingest th√™m data m·ªõi (kh√¥ng c·∫ßn RunPod n·∫øu d√πng OpenAI)
python scripts/ingest.py --llm openai --data-dir ./new_data

# Xem logs LightRAG
tail -f lightrag.log

# Connect RunPod khi c·∫ßn re-index l·ªõn
runpodctl start pod <pod-id>
ssh root@<runpod-ip> -p <port>
```

---

## L∆ØU √ù QUAN TR·ªåNG

1. **Embedding model ph·∫£i nh·∫•t qu√°n:** Ch·ªçn 1 model t·ª´ ƒë·∫ßu (OpenAI `text-embedding-3-small` ho·∫∑c `bge-m3`). N·∫øu ƒë·ªïi model ‚Üí ph·∫£i x√≥a to√†n b·ªô vector data v√† re-index.

2. **Ti·∫øng Vi·ªát:** ƒê√£ set `language: Vietnamese` trong `addon_params` ‚Üí LightRAG s·∫Ω extract entities b·∫±ng ti·∫øng Vi·ªát.

3. **Context window:** Qwen2.5:32B c·∫ßn `num_ctx: 32768` (32k tokens), kh√¥ng ph·∫£i 8k m·∫∑c ƒë·ªãnh.

4. **RunPod cost:** RTX 4090 tr√™n RunPod ~$0.44/gi·ªù. Ch·ªâ b·∫≠t khi c·∫ßn, **nh·ªõ t·∫Øt** sau khi xong!

5. **Data privacy:** Mock data n√†y an to√†n. Khi d√πng data th·∫≠t, c√¢n nh·∫Øc d√πng Ollama local 100% ƒë·ªÉ kh√¥ng g·ª≠i data ra ngo√†i.
